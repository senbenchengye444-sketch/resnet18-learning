# ResNet18 転移学習実験（CIFAR-10）

## 1. 概要

本実験では、ImageNetで事前学習されたResNet18を用い、CIFAR-10に対する転移学習を段階的に実施した。特に、**どの層まで解凍（fine-tuning）することが有効か**に着目し、以下の3条件を比較した。

* 全結合層（fc）のみ学習
* fc + layer4 を解凍
* fc + layer3 + layer4 を解凍

また、epoch数を10および50とした場合の挙動の違いも検証した。

---

## 2. 実験設定

### 2.1 データセット

* CIFAR-10
* 学習データ数：50,000
* テストデータ数：10,000
* 画像サイズ：32×32（ResNet入力に合わせ224×224にリサイズ）

### 2.2 前処理

* Resize: 224×224
* RandomHorizontalFlip（学習時のみ）
* Normalize（ImageNet mean / std）

### 2.3 モデル

* ResNet18（ImageNet事前学習済み）
* 出力層を10クラス用に変更

### 2.4 学習設定

* Optimizer: Adam
* Loss: CrossEntropyLoss
* 学習率（層ごと）:

  * fc: 1e-3
  * layer4: 1e-4
  * layer3: 1e-5（解凍時のみ）

---

## 3. 実験内容


### 実験1：fc + layer4 解凍

* 凍結層：conv1〜layer3
* 学習層：layer4, fc

### 実験2：fc + layer3 + layer4 解凍

* 凍結層：conv1〜layer2
* 学習層：layer3, layer4, fc

それぞれについて epoch=10 および epoch=50 で学習を行った。

---

## 4. 実験結果

### 4.1 epoch = 10 の結果

| 学習条件                 | Test Accuracy |
| -------------------- | ------------- |
| fc + layer4          | 91.33%        |
| fc + layer3 + layer4 | 92.30%        |

---

### 4.2 epoch = 50 の結果

| 学習条件                 | Loss   | Test Accuracy |
| -------------------- | ------ | ------------- |
| fc + layer4          | 0.0130 | 92.01%        |
| fc + layer3 + layer4 | 0.0057 | 93.35%        |

学習回数を増やしても、いずれの条件でもlossは安定して減少し、急激な性能劣化（崩壊）は観測されなかった。

---

## 5. 考察

### 5.1 layer4 解凍の効果

layer4 はクラス判別に直結する高次特徴を担っており、CIFAR-10に適応させることで大幅な精度向上が得られた。

### 5.2 layer3 解凍が有効だった理由

* CIFAR-10はクラス数が少なくタスクが比較的単純
* データ数が十分（50,000）
* 学習率を1e-5と極めて小さく設定したため、汎用特徴を破壊せず微調整が可能だった

その結果、epoch=50においてもrepresentation driftによる性能劣化は確認されなかった。

### 5.3 一般論との関係

一般に浅い層の解凍は汎用特徴を破壊するリスクがあるとされるが、本実験は「**適切な条件下では layer3 までの解凍が有効になり得る**」ことを示している。ただし、改善幅は限定的であり、再現性・安全性の観点からは layer4 までの解凍が最も安定した戦略と考えられる。

---

## 6. 結論

* 転移学習では「どこまで解凍するか」が性能に大きく影響する
* fc + layer4 解凍は安定して高精度を達成できる
* layer3 解凍は、学習率・epoch・データ量を慎重に制御すれば有効な場合がある
* 一般解としては layer4 までの部分的ファインチューニングが推奨される

---

## 7. 今後の課題

* データ数を減らした場合の挙動検証
* CIFAR-100 等、より難易度の高いタスクでの再現性確認
* 学習曲線（loss / accuracy）の可視化による詳細分析
